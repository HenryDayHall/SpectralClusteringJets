\section{Theory of spectral clustering}\label{sec:spectral_theory}

{\color{magenta} Collimated emissions of particles are clustered by jet algorithms.  A
representation of observable particles that preserves and accentuates local information
motivates the Laplacian eigenmap~\cite{BelkinNiyogi2003} and spectral
clustering~\cite{NgJordanWeiss2002}.}  
Spectral clustering is a method by which a set of points are represented in a new space,
called the embedding space, in which they can be easily clustered.  Coordinates of the
points in the embedding space are expressed in terms of the eigenvectors and eigenvalues
of an associated Laplacian matrix, hence the name.

%
{\color{magenta} The particles in an event are described first as nodes of a graph and
edges capturing a notion of similarity between them.}  The theory behind the
construction of the embedding space is a relaxation of criteria that would precisely
partition nodes into separate disconnected subgraphs.
\sout{This criteria can then be relaxed to permit some connections between subgraphs.}
An excellent description can be found in~\cite{luxburg2007spectraltutorial}; a short
summary is given here.

At the start we have a group of points with coordinates, which should be split into a  number \(c\) of predetermined clusters.
Applying the spectral clustering method requires making these points into a graph.
A simple way to do this would be to consider the points to be the nodes of a fully connected graph.
The vertex of the graph joining node (or point) \(i\) and \(j\) has weight \(a_{i, j}\),
which should grow with the probability of \(i\) and \(j\) being in the same group.

The initial aim is to identify which of the components each point belongs to,
by sorting the graph into subgraphs, \(G_k\), where \(k=1 \dots c\).
Minimising the NCut objective is a function that captures this aim, where 
\begin{equation}
    \text{NCut} = \frac{1}{2}\sum_k\frac{W(G_k, \bar{G_k})}{\text{vol}(G_k)},
\end{equation}\label{eqn:cost_function}
where \(W(G_k, \bar{G_k})\) is the sum of all the vertex weights that must be dropped
to separate the cluster \(G_k\) from the rest of the graph, \(\bar{G_k}\).
So that \( W(G_k, \bar{G_k}) = \sum_{i \in G_k, j \in \bar{G_k}} a_{i, j} \).
In the denominator \(\text{vol}(G_k) = \sum_{i \in G_k} \sum_{j} a_{i, j}\),
the sum of all affinities connecting to a point in \(G_k\).
This denominator is used to penalise forming small clusters.

In order to determine which point will go in which \(G_k\), a set of indicator vectors must be found.
Membership of cluster \(G_k\) will be recorded in the indicator vector \(h_k\):
\begin{equation}\label{eqn:indicator}
    h_{i, k}= 
    \begin{cases}
        1/\sqrt{\text{vol}(G_k)}& \text{if point } i \in G_k ,\\
        0             & \text{otherwise},
    \end{cases}
    .
\end{equation}

To find these indicator vectors the graph is represented by the graph Laplacian, \(L\), a square
matrix with as many rows and columns as there are points.
To construct this Laplacien we define two other matrices;
an off diagonal matrix 
\(A_{i, j} = (1 - \delta_{i, j})a_{i, j}\)
and a diagonal matrix
\(D_{i, j} = \delta_{i, j}\sum_q a_{i, q}\).
Then the symmetric Laplacian can be simply written as;
\begin{equation}\label{eqn:symmetric_laplacian}
    L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}
\end{equation}

Notice that this is a real symmetric matrix
and, therefore all its eigenvalues are real.
Considering just one cluster, \(G_k\), when the Laplacian is multiplied by its indicator vector,
the result is the term that NCut seeks to minimise for that cluster.
\begin{equation}
    h_k'Lh_k = \frac{1}{\text{vol}(G_k)}\sum_{i \in G_k, j \in G_k} \left(\delta_{i, j}\sum_{l} a_{l, i} - a_{i, j} \right) = \frac{W(G_k, \bar{G_k})}{\text{vol}(G_k)}
\end{equation}
To obtain the sum of all the terms, stack the indicator vectors into a matrix,
\( h'_k L h_k = (H'L H)_{kk}\),
and the NCut aim described earlier becomes the trace,
\begin{equation} \text{NCut}(G_1,G_2, \dots G_n) \equiv \frac{1}{2} \sum_{k=1}^n \frac{W(G_k, \bar{G_k})}{\text{vol}(G_k)} = \text{Tr}(H'LH),\end{equation}
where \(H'H = I\).
This is still an NP hard problem, however if we relax the requirements made on \(h\) in Eqn.~\ref{eqn:indicator},
allowing the elements of \(h\) to take arbitrary values, then the Rayleigh-Ritz theorem provides a solution.
Trace minimisation in this form is done
by finding the eigenvectors of \(L\) with smallest 
eigenvalues.
Due to the form of the Laplacian, there will be an eigenvector with components all of the same value and its eigenvalue will be \(0\).
This corresponds to the trivial solution of considering all points to be in one group.
The next \(c\) eigenvectors of \(L\), sorted by smallest eigenvector, are the indicator vectors needed to allocate points to \(c\) clusters.

These indicator vectors are then used to determine position of the points in the embedding space.
Each indicator vector has as many elements as there are points to be clustered,
so the coordinates of a point are the corresponding elements or the indicator vectors.
This is all the information the theory of spectral clustering provides.
The steps required to make use of this information are not dictated by the theory,
and they must be carefully selected to respect the physics.

Using the positions in embedding space the points can be gathered agglomeratively,
so that we do not need to chose a predetermined number of clusters.

\subsection{Distance in the embedding space}\label{sec:embedding_distance}
%Elements of the eigenvector indicate which points belong in the same group, although 
%they do not signify which points should be separated.
%This can be verified by looking at the cost function in Eq.~(\ref{eqn:cost_function}),
%it has a term to penalise separating points that should be together, 
%but no term for joining points that should be separated.
%In many configurations, indicator vectors will be indicating multiple groups.
%If the ideal grouping of some set was \(G_1\), \(G_2\) and \(G_3\),
%then
%\begin{equation}
%\hspace*{-0.25truecm}
%\begin{aligned}
%    \text{RatioCut}(G_1, G_2, G_3) = &
%    \frac{W(G_1\cup G_2, G_3)}{|G_3|} + \frac{W(G_2\cup G_3, G_1)}{|G_1|} + \frac{W(G_1\cup G_3, G_2)}{|G_2|} \\
%    >& \\
%    \text{RatioCut}(G_1\cup G_2, G_2\cup G_3, G_1\cup G_3) =&
%    \frac{W(G_1\cup G_2, G_3)}{|G_1\cup G_2|} + \frac{W(G_2\cup G_3, G_1)}{|G_2\cup G_3|} + \frac{W(G_1\cup G_3, G_2)}{|G_1\cup G_3|} \\
%\end{aligned}
%\end{equation}
%
%Note that this still uniquely determines the group for each point,
%however, we do not expect one group per indicator vector, 
%or in the relaxed case, eigenvector.
When the relaxed spectral clustering algorithm is used to create an embedding space, points in a group will not be at exactly the same coordinates.
Each point can be seen as a vector, the direction of this vector indicates the group to which this point should be assigned.
The magnitude indicates the confidence with which the assignment is made.
Changes in magnitude cause the Euclidean distance between the corresponding points to grow.
An angular distance is appropriate, though. 
The angular distance will grow when the eigenvectors indicating the point have less overlap
and this is what should be measured.

\subsection{Information in the eigenvalues}\label{sec:eig_norm}
When the clusters in the data are very clear, the situation is closer to the ideal one and the eigenvalues will be closer to \(0\).
The smaller an eigenvalue is, the more like a perfect indicator vector the corresponding eigenvector is.
It is possible to make use of this information.

In a traditional application of spectral clustering, the number of clusters desired, \(c\), is predetermined.
The embedding space is created by taking \(c\) eigenvectors with smallest eigenvalues, excluding the trivial eigenvector.
The embedding space then has \(c\) dimensions.
This follows from a relaxation of the concept of indicator vectors.

When forming jets we do not know from the outset how many clusters to expect in the dataset,
so the number of eigenvectors to keep is not clear.
While we could chose a fixed, arbitrary number of eigenvectors, this is suboptimal.
A better approach is to take all non-trivial eigenvectors corresponding to eigenvalues
smaller than some limiting number, \(\lambda_\text{limit}\).
For a symmetric matrix the eigenvalues will be \(0 < \lambda < 2\),
so \(\lambda_\text{limit} = 0.5\) would be a reasonable choice.
Then, the number of dimensions in the embedding space will vary,
according to the number of non-trivial eigenvectors with corresponding \(\lambda < \lambda_\text{limit}\).

There is one more manipulation from the information in the eigenvalues.
The dimensions of this embedding space are not of equal importance,
those with higher eigenvalues being less interesting.
This can be accounted for by dividing the eigenvector by some power, \(\beta\), of the eigenvalue.

Let the eigenvectors for which \(\lambda < \lambda_\text{limit}\) be
\begin{equation}
    L_{i, j} (h_n)_j = \lambda_n (h_n)_i.
\end{equation}
Then, the coordinates of the \(j^\text{th}\) point in the \(c\) dimensional embedding space
become \(m_j = \left(\lambda_1^{-\beta} (h_1)_j, \dots \lambda_c^{-\beta} (h_c)_j,\right)\).
In effect the \(n^\text{th}\) dimension is compressed by a factor \(\lambda_n^\beta\),
so the larger \(\lambda_n\) the greater the compression.

\subsection{Stopping conditions}\label{sec:stopping_condintion}

If a recursive algorithm is to be chosen, like in the \genkt{} algorithm, a stopping condition is needed.
A stopping condition based on smallest distance between points in the embedding space does not prove to be stable,
as the distribution  in the number of dimensions in the embedding space changes sharply from event to event.

The average distance between points is more stable.
If this were used in physical space it would force roughly the same number of clusters to form each time,
however, the variable number of dimensions in the embedding space is now an advantage.
The  clearer information found about clusters in the points the more dimensions the embedding space will contain,
as described in section~\ref{sec:eig_norm}.

Say, the data contains two points that would form a good cluster.
If those two points are combined into one, that cluster is complete,
fewer clusters remain unfinished
and the information for clustering the resulting points will be reduced.
When the embedding space is recalculated for the new points, it will likely have fewer dimensions.
In a space with fewer dimensions the mean distance between the points naturally falls.
Thus, the mean distance in the embedding space is a good indicator of the number of unfinished clusters available.
In short, the mean distance in the embedding space makes a natural cut-off.
