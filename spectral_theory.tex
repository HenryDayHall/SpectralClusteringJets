\subsection{Theory of spectral clustering}\label{sec:spectral_theory}

Spectral clustering is a method by which a set of points are represented in a new space,
in which they can be easily clustered.
This space is called the embedding space.
Locations of the points in the embedding space is determined by calculating a
spectrum of eigenvalues, hence the name.
In this new space the points are arranged such that they are easier to cluster.

The theory behind the construction of the embedding space is a relaxation of criteria that would precisely
separate disconnected subgraphs.
This criteria can then be relaxed to permit some connection between subgraphs.
An excellent description can be found in~\cite{luxburg2007spectraltutorial}, a short summary is given here.

At the start we have a group of points with coordinates, which should be split into a \(c\) predetermined clusters.
Applying the spectral method requires making these points into a graph.
A simple way to do this would be to consider the points to be the nodes of a fully connected graph.
The vertex of the graph joining node (point) \(i\) and \(j\) has weight \(a_{i, j}\) where
\begin{equation}
    a_{i, j}= 
    \begin{cases}
        1, & \text{if points } i \text{ and } j \text{ are closer than } \delta \\
        0,              & \text{otherwise}
    \end{cases}
    .
\end{equation}

If points that should go in different clusters are always separated by more than \(\delta\),
and points inside a cluster always have at least one neighbour closer than \(\delta\),
then this describes a set of disconnected components, one for each cluster.

The initial aim is to identify which of the components each point belongs to,
by sorting the graph into subgraphs, \(A_k\), where \(k=1 \dots c\).
Minimizing the NCut objective is a function that captures this aim; 
\begin{equation}
    \text{NCut} = \sum_k\frac{W(A_k, \bar{A_k})}{|A_k|}
\end{equation}
Where \(W(A_k, \bar{A_k})\) is the sum of all the vertex weights that must be dropped
to separate the cluster \(A_k\) from the rest of the graph, \(\bar{A_k}\).
So that \( W(A_k, \bar{A_k}) = \sum_{i \in A_k, j \in \bar{A_k}} a_{i, j} \).
In the denominator \(|A_k|\) corresponds to the number of elements in \(A_k\).
This denominator is used to penalise forming small clusters.

In order to determine which point will go in which \(A_k\) a set of indicator vectors must be found.
Membership of cluster \(A_k\) will be recorded in the indicator vector \(h_k\);
\begin{equation}
    h_{i, k}= 
    \begin{cases}
        1/\sqrt{|A_k|},& \text{if point } i \in A_k \\
        0,              & \text{otherwise}
    \end{cases}
    .
\end{equation}

To find these indicator vectors the graph is represented by the graph Laplacian, a square
matrix with as many rows and columns as there are points.
On \(i\)th diagonal it holds the value \(\sum_j a_{i, j}\),
which is the degree of the vertex \(i\).
Off the diagonal, in row\(i\) column \(j\), it has the negative of the vertex weight between \(i\) and \(j\), \(-a_{i, j}\).
\begin{equation}\label{eqn:laplacian}
    L = 
    \begin{pmatrix}
        z_0 & -a_{0,1} & -a_{0,2} & \hdots \\
        -a_{0,1} & z_1 & -a_{1,2} & \\
        -a_{0,2} & -a_{1,2} & z_2 & \\
        \vdots   &          &     & \ddots 
    \end{pmatrix}
\end{equation}

Notice that this is a real symmetric matrix,
and therefore all its eigenvalues are real.
Considering just one cluster, \(A_k\), when the Laplacian is multiplied by it's indicator vector,
the result is the term that NCut seeks to minimise for that cluster.
\begin{equation}
    h_k'Lh_k = \frac{1}{|A_k|}\sum_{i \in A_k, j \in A_k} \left(\delta_{i, j}\sum_{l} a_{l, i} - a_{i, j} \right) = \frac{W(A_k, \bar{A_k})}{|A_k|}
\end{equation}
To obtain the sum of all the terms, stack the indicator vectors into a matrix;
\begin{equation} h'_k L h_k = (H'L H)_{kk}\end{equation}
and the NCut aim described earlier becomes the trace;
\begin{equation} \text{NCut}(A_1,A_2, \dots A_n) \equiv \frac{1}{2} \sum_{i=1}^n \frac{W(A_i, \bar{A_i})}{|A_i|} = \text{Tr}(H'LH)\end{equation}
Where \(H'H = I\).
Trace minimisation in this form is done
by finding the eigenvectors of \(L\) with smallest 
eigenvalues.
Due to the form of the Laplacien, there will be an eigenvector with components of all the same value, and it's eigenvalue will be \(0\).
This corresponds to the trivial solution of considering all points to be in one group.
The next \(c\) eigenvectors of \(L\), sorted by smallest eigenvector, are the indicator vectors needed to allocate points to \(c\) clusters.

Generalising this to a graph where the vertices can have weights other than \(1\) or \(0\)
only requires relaxing the requirements on the form of the indicator vectors; \(h_k\).

These indicator vectors are then used to determine position of the points in the embedding space.
Each indicator vector has as many elements as there are points to be clustered,
so the coordinates of a point are the corresponding elements or the indicator vectors.
Using the positions in embedding space the points can be gathered agglomeratively,
so that we do not need to chose a predetermined number of clusters.
This last step is not a relaxation of a well defined objective.
